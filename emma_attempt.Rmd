---
title: "ACM Portfolio 2 - Reinforcement Learning Agent"
author: "EOL"
date: "2024-02-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Reinforcement Learning Agent
We define 3 functions: 

The first is the `softmax function` used to calculate the probability of choosing a particular hand at a given trial. At a given trial, the agent's choice is probabilistically determined by a softmax function that calculates the probability of choosing a particular hand (left or right) based on the current expected values of choosing left or right, respectively. The temperature parameter in the softmax function, theta, determines the agent's level of exploiration vs. exploitation. A higher temperature value leads to more exploration, while a lower temperature value leads to more exploitation. So the `softmax function` converts the expected values into probabilities that sum up to 1. (NB: could also be exchanged with a RELU function we could try???).

```{r softmax function}

softmax_f <- function(exp_values, theta) {
  probabilities <- exp(exp_values * theta) / sum(exp(exp_values * theta))
  return(probabilities) 
}

```

Moreover, the expected values are updated at each trial using the Rescorla-Wagner learning rule, which is a simple model of associative learning. According to the Rescorla-Wagner rule, the expected values are updated based on the prediciton error on the previous trial, which is the difference between the outcome (i.e., feedback) and the expected value. The learning rate, alpha, determines the extent to which the expected values are updated at each trial.

Below, the Rescorla Wagner learning rule is implemented. It updates the expected value for the chosen option based on the received feedback, using the prediction error and a learning rate.

The function contains the following elements:
- `value`: a vector of the expected values for the two options (0/1, corresponding to left and right). The values are updated at each trial based on the feedback received. We can access value[1] and value[2].
- `alpha`: the learning rate, which determines the extent to which the expected values are updated at each trial. We define it later when we simulate the data.
- `choice`: the choice made at the current trial, which is a binary value (0/1, corresponding to left and right).
- `feedback`: received for the choice made in the previous trial, and it is a scalar value indicating the outcome (0 for incorrect, 1 for correct).
- `PE`: the prediction error, which is the difference between the feedback and the expected value of the chosen option. I.e., how much the agent was "surprised" by the feedback. 
- `value[1]`: represents the expected value of choosing the left hand
- `value[2]`: represents the expected value of choosing the right hand.


```{r Rescorla-Wagner learning rule}

ValueUpdate_f <- function(value, alpha, choice, feedback) {
  PE <- feedback - value
  
  v1 <- value[1] + alpha * (1 - choice) * (feedback - value[1])
  v2 <- value[2] + alpha * choice * (feedback - value[2])
  
  updatedValue <- c(v1, v2)
  
  return(updatedValue)
}

```


The main agent function that simulates the agent's choices and updates the expected values at each trial.

The function below contains the following parameters:
- `value`: a vector of the expected values for the two options (0/1, corresponding to left and right). The values are updated at each trial based on the feedback received. 
- `alpha`: the learning rate, which determines the extent to which the expected values are updated at each trial. We define it later when we simulate the data. 
- `feedback`: received for the choice made in the previous trial, and it is a scalar value indicating the outcome (0 for incorrect, 1 for correct).

The other elements in the function are:
- `choice_probabilities`: the probability of choosing each option at the current trial, calculated using the softmax function. The variable type is a vector of length 2. 
- `choice`: the choice made at the current trial, which is sampled from a binomial distribution with size 1 and the probabilities calculated using the softmax function.
- `updatedValue`: the updated expected values for the two options, based on the choice made and the feedback received. The variable type is a vector of length 2.

```{r Agent Function}

RL_agent_f <- function(value, alpha, theta, feedback) {

  # here we use softmax to decide the probability of choosing each option
  choice_probabilities <- softmax_f(value, theta)
  
  # make a choice based on the calculated probabilities outputted from softmax
  choice <- sample(c(0, 1), size = 1, prob = choice_probabilities)
  
  # update values based on the choice made and feedback received
  updatedValue <- ValueUpdate_f(value, alpha, choice, feedback)
  
  # return the choice and the updated values
  return(list(choice = choice, updatedValue = updatedValue))
}
  
  
```


## Noisy WSLS Agent 
```{r Noisy WSLS Agent}

# WSLS Agent function with noise 
WSLSAgentNoise_f <- function(prevChoice, Feedback, noise){

  if (Feedback == 1) {
    choice <- prevChoice
  } else if (Feedback == 0) {
    choice <- 1 - prevChoice
  }
  
  # applying noise
  if (rbinom(1, 1, noise) == 1) {
    choice <- rbinom(1, 1, 0.5)
  }
  
  return(choice)
}
```

## Simulating Data: The Two Agents Playing Against Each Other, One game is 120 trials, they play 20 games.

```{r, old - df instead of lists}
# parameters for the RL Agent
alpha <- 0.1  # learning rate
theta <- 0.5  # temperature parameter for softmax

# parameters for the noisy WSLS Agent
noise <- 0.1  # probability of choosing randomly due to noise

# number of games and trials
num_games <- 20
num_trials <- 120

# store results
results <- list()

for (game in 1:num_games) {
  # initial values for both agents
  RL_value <- c(0.5, 0.5)  # initial expected values for RL Agent
  WSLS_prevChoice <- sample(c(0, 1), size = 1)  # initial choice for WSLS Agent
  WSLS_Feedback <- 0  # initial feedback (assume loss)
  
  game_results <- list()
  
  for (trial in 1:num_trials) {
    # RL agent makes a choice and updates values
    RL_outcome <- RL_agent_f(RL_value, alpha, theta, WSLS_Feedback)
    RL_choice <- RL_outcome$choice
    RL_value <- RL_outcome$updatedValue
    
    # noisy WSLS Agent makes a choice
    WSLS_choice <- WSLSAgentNoise_f(WSLS_prevChoice, WSLS_Feedback, noise)
    
    # determine feedback based on choices (simple rule: same choice = win, different choice = lose)
    if (RL_choice == WSLS_choice) {
      RL_Feedback <- 1  # Win
      WSLS_Feedback <- 1
    } else {
      RL_Feedback <- 0  # Lose
      WSLS_Feedback <- 0
    }
    
    # update WSLS's previous choice
    WSLS_prevChoice <- WSLS_choice
    
    # record trial results
    game_results[[trial]] <- list(RL_choice = RL_choice, WSLS_choice = WSLS_choice, RL_Feedback = RL_Feedback, WSLS_Feedback = WSLS_Feedback)
  }
  
  results[[game]] <- game_results
}

```

```{r, new - lists instead of df}
# Simulating Data: 
# The Two Agents Playing Against Each Other
# One game is 120 trials, they play 20 games.

set.seed(1984)

alpha <- 0.1 # RL
theta <- 0.5 # RL
noise <- 0.1 # WSLS

num_games <- 20 
num_trials <- 120

# initialize lists to store data
RL_data <- list()
WSLS_data <- list()

# simulate interactions for multiple games
for (game in 1:num_games) {
  # empty vectors to store choices 
  RL_choices <- vector("list", length = num_trials)
  WSLS_choices <- vector("list", length = num_trials)
  RL_feedback <- rep(NA, num_trials)
  WSLS_feedback <- rep(NA, num_trials)
  RL_value <- c(0.5, 0.5)

  # simulate interactions 
  for (i in 1:num_trials) {
    if (i == 1) {
      # initialize first choices randomly for both agents 
      RL_choices[[i]] <- sample(c(0,1), 1)
      WSLS_choices[[i]] <- sample(c(0,1), 1)    

      if (RL_choices[[i]] == WSLS_choices[[i]]) {
        RL_feedback[i] <- 1  # Win
        WSLS_feedback[i] <- 0
      } else {
        RL_feedback[i] <- 0  # Lose
        WSLS_feedback[i] <- 1
      }

    } else {

      # RL agent makes a choice and updates values
      RL_outcome <- RL_agent_f(RL_value, alpha, theta, WSLS_feedback[i-1])
      RL_choice <- RL_outcome$choice
      RL_value <- RL_outcome$updatedValue
      WSLS_prevChoice <- WSLS_choices[[i-1]]

      # noisy WSLS Agent makes a choice
      WSLS_choice <- WSLSAgentNoise_f(WSLS_prevChoice, WSLS_feedback[i-1], noise)

      # determine feedback based on choices (simple rule: same choice = win, different choice = lose)
      if (RL_choice == WSLS_choice) {
        RL_feedback[i] <- 1  # Win
        WSLS_feedback[i] <- 0
      } else {
        RL_feedback[i] <- 0  # Lose
        WSLS_feedback[i] <- 1
      }

      # record trial results
      RL_choices[[i]] <- RL_choice
      WSLS_choices[[i]] <- WSLS_choice
    }
  }

  # append results to lists
  RL_data[[game]] <- list(n = num_trials, h = RL_choices, feedback = RL_feedback)
  WSLS_data[[game]] <- list(n = num_trials, h = WSLS_choices, feedback = WSLS_feedback)
}

# save data as lists
save_data <- list(RL_data = RL_data, WSLS_data = WSLS_data)

```


# Analyze the results

```{r}
library(ggplot2)
library(dplyr)
```

```{r}
# extract data for plotting
results_df <- data.frame(
  Game = rep(1:num_games, each = num_trials),  
  Trial = rep(1:num_trials, times = num_games),  
  RL_Choice = numeric(num_games * num_trials),
  WSLS_Choice = numeric(num_games * num_trials)
)

```

```{r}
for (trial in 1:num_trials) {
  results_df$RL_Choice[trial] <- results[[1]][[trial]]$RL_choice
  results_df$WSLS_Choice[trial] <- results[[1]][[trial]]$WSLS_choice
}
```

```{r}
library(ggplot2)

ggplot(results_df[results_df$Game == 1, ], aes(x = Trial)) +
  geom_line(aes(y = RL_Choice, color = "RL Agent"), size = 0.3) +
  geom_line(aes(y = WSLS_Choice, color = "Noisy WSLS Agent"), size = 0.3) +
  labs(title = "Choice Patterns in Game 1: RL Agent vs. Noisy WSLS Agent", y = "Choice", x = "Trial") +
  scale_color_manual(name = "Agents", values = c("RL Agent" = "blue", "Noisy WSLS Agent" = "red")) +
  theme_minimal()

```

```{r}
library(ggplot2)
library(dplyr)

# Initialize an empty list to store cumulative feedback for each game
cumulative_feedback <- list()

# Iterate over each game to calculate cumulative feedback
for (game in 1:num_games) {
  cumulative <- 0
  feedback <- numeric(num_trials)
  
  # Iterate over each trial in the game
  for (trial in 1:num_trials) {
    # Add feedback of the current trial to cumulative feedback
    cumulative <- cumulative + results[[game]][[trial]]$RL_Feedback
    feedback[trial] <- cumulative
  }
  
  cumulative_feedback[[game]] <- feedback
}

# Create a data frame to store the cumulative feedback for each game
feedback_df <- data.frame(
  Game = rep(1:num_games, each = num_trials),
  Trial = rep(1:num_trials, times = num_games),
  Cumulative_Feedback = unlist(cumulative_feedback)
)

# Plot cumulative feedback for RL agent across all games
ggplot(feedback_df, aes(x = Trial, y = Cumulative_Feedback, color = as.factor(Game))) +
  geom_line() +
  labs(title = "Cumulative Feedback for RL Agent Across Games",
       x = "Trial Number",
       y = "Cumulative Feedback") +
  theme_minimal() +
  scale_color_discrete(name = "Game")  # Set color scale for different games


```







