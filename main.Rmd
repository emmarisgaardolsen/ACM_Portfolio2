---
title: "Portfolio_2_ACM_JW"
output: html_document
date: "2024-02-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(tidyverse)
```

# Portfolio 2

**Model**: Reinforcement learning Model

## Functions

The following chunk contains four functions

1. Activation function
2. Value update function
3. RL Agent function
4. Noisy WSLS agent function

The activation function is a softmax and is used to decide the probability of choosing each option. The value update function is designed to update the values of a two-element vector based on a learning rate (alpha), a binary decision (choice), and some feedback from previous trials. The agent function is a reinforcement learning function that uses softmax to decide the probability of choosing each option, makes a choice based on the calculated probabilities outputted from softmax, and updates values based on the choice made and feedback received. The last function is a noisy WSLS agent function that is designed to simulate a noisy win-stay-lose-shift agent against which the reinforcement learning agent can play against.

```{r activation functions}
###########################
### Activation function ###
###########################
softmax <- function(x, tau) {
  outcome = 1 / (1 + exp(-tau * x))
  return(outcome)
}

#############################
### Value update function ###
#############################
ValueUpdate = function(value, alpha, choice, feedback) {
  
  PE <- feedback - value # the prediction error 
  
  v1 <- value[1] + alpha * (1 - choice) * (feedback - value[1]) # value 1 = current estimate of some value, feedback = new information received, alpha = learning rate, choice is the action taken

  v2 <- value[2] + alpha * (choice) * (feedback - value[2]) 
  
  updatedValue <- c(v1, v2)
  
  return(updatedValue)
}

########################################################
### Agent function (reinforcement learning function) ###
########################################################
RL_agent_f <- function(value, alpha, theta, feedback) {

  # here we use softmax to decide the probability of choosing each option
  choice_probabilities <- softmax(value, theta)
  
  # make a choice based on the calculated probabilities outputted from softmax
  choice <- sample(c(0, 1), size = 1, prob = choice_probabilities)
  
  # update values based on the choice made and feedback received
  updatedValue <- ValueUpdate(value, alpha, choice, feedback)
  
  # return the choice and the updated values
  return(list(choice = choice, updatedValue = updatedValue))
}

############################################
### Noisy WSLS agent function (function) ###
############################################
WSLSAgentNoise_f <- function(prevChoice, Feedback, noise){

  if (Feedback == 1) {
    choice <- prevChoice
  } else if (Feedback == 0) {
    choice <- 1 - prevChoice
  }
  
  # applying noise
  if (rbinom(1, 1, noise) == 1) {
    choice <- rbinom(1, 1, 0.5)
  }
  
  return(choice)
}
```

## Simulation of data with the two agents
```{r data sim}
### parameters for the RL Agent
alpha <- 0.1  # learning rate
theta <- 0.5  # temperature parameter for softmax

#### parameters for the noisy WSLS Agent
noise <- 0.1  # probability of choosing randomly due to noise

#### number of games and trials
num_trials <- 1000

# store results
results <- list()

# initialization 
value <- c(0.5, 0.5) #values for the RL agent
prevChoice <- sample(c(0,1), size = 1) #previous choice for the WSLS agent
WSLS_Feedback <- 0 # initial feedback for WSLS agent (assume loss)

# simulation
for (t in 1:num_trials) {
  
  # call the RL agent function
  RL_outcome <- RL_agent_f(value = value, alpha = alpha, theta = theta, feedback = WSLS_Feedback)
  
  # update value and choice for RL Agent
  value <- RL_outcome$updatedValue
  RL_Choice <- RL_outcome$choice
  
  # WSLS agent makes its decision
  WSLS_Choice <- WSLSAgentNoise_f(prevChoice = prevChoice, Feedback = WSLS_Feedback, noise = noise)
  
  # feedback for the RL and WSLS agents; same choice = win, different choice = lose
  if (RL_Choice == WSLS_Choice) {
    RL_Feedback <- 1  # Win
    WSLS_Feedback <- 0
  } else {
    RL_Feedback <- 0  # Lose
    WSLS_Feedback <- 1
  }
  
  # updating the previous choice made by the WSLS agent
  prevChoice <- WSLS_Choice

  # store results
  results[[t]] <- data.frame(RL_Choice = RL_Choice, RL_Feedback = RL_Feedback, WSLS_Choice = WSLS_Choice, WSLS_Feedback = WSLS_Feedback)   
}

# convert results to data frame
results_df <- do.call(rbind, results)
```

## Making data ready for stan
```{r}
# converting results to integer lists
RL_Choice <- as.integer(results_df$RL_Choice)
RL_Feedback <- as.integer(results_df$RL_Feedback)
WSLS_Choice <- as.integer(results_df$WSLS_Choice)
WSLS_Feedback <- as.integer(results_df$WSLS_Feedback)
```

## Modelling with stan (using rstan and not cmdstanr or brms)
```{r}
pacman::p_load(rstan)

# Compiling model
model <- stan_model("model_RL.stan")
```

```{r}
# Fitting the model
# parallelization
options(mc.cores = parallel::detectCores())
#options(mc.cores = 4) # allows for parallel processing

fit <- sampling(model, 
                data = list(N = num_trials, RL_Choice = RL_Choice, RL_Feedback = RL_Feedback, WSLS_Choice = WSLS_Choice, WSLS_Feedback = WSLS_Feedback), # the data
                seed = 12345, # seed
                chains = 4, # number of chains
                iter = 1000, # total number of iterations (including warmup)
                warmup = 1000,
                # tree depth
                control = list(max_treedepth = 20, adapt_delta = 0.99)) #tree depth =  how many steps in the future to check to avoid u-turns; adapt_delta = # how high a learning rate to adjust hyperparameters during warmup
```